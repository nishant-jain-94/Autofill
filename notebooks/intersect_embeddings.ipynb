{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import os\n",
    "import numpy as np\n",
    "import json\n",
    "import pickle\n",
    "import datetime\n",
    "import spacy\n",
    "from keras.utils import to_categorical\n",
    "from nltk.tokenize import word_tokenize,sent_tokenize\n",
    "from load_squad_wiki_data import get_squad_data, get_squad_wiki_data\n",
    "from gensim.models import Word2Vec\n",
    "from spacy.en import English\n",
    "nlp = spacy.load('en', parser=False, matcher=False, add_vectors=False)\n",
    "nlp_en = English()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class MakeIter(object):\n",
    "    def __init__(self, generator_func, **kwargs):\n",
    "        self.generator_func = generator_func\n",
    "        self.kwargs = kwargs\n",
    "    def __iter__(self):\n",
    "        return self.generator_func(**self.kwargs)\n",
    "    \n",
    "class Embeddings:\n",
    "    def __init__(self, size, window, min_count, workers):\n",
    "        \n",
    "        self.size = size\n",
    "        self.window = window\n",
    "        self.min_count = min_count\n",
    "        self.workers = workers\n",
    "        base_file_name = '_'.join([str(number) for number in [size, window, min_count, workers]])\n",
    "        self.path_word2vec_model = '../data/word2vec_model_{0}.pickle'.format(base_file_name)\n",
    "        self.path_word_tokenized_sentence = '../data/word_tokenized_sentence_{0}.json'.format(base_file_name)\n",
    "        self.path_indexed_sentences = '../data/indexed_sentences_{0}.json'.format(base_file_name)\n",
    "        self.path_vocabulary = '../data/vocabulary_{0}.json'.format(base_file_name)\n",
    "        self.path_google_intersected = '../data/google_intersected_model_{0}.pickle'.format(base_file_name)\n",
    "        self.index_sentences()\n",
    "    \n",
    "    def tokenize_sentences(self):\n",
    "        tokenized_sentences = []\n",
    "        embeddings_generator = self.load_embeddings()\n",
    "        for sentence in embeddings_generator:\n",
    "            tokenized_sentence = word_tokenize(sentence.lower())\n",
    "            tokenized_sentences.append(tokenized_sentence)\n",
    "        with open(self.path_word_tokenized_sentence, \"w\") as outfile:\n",
    "            json.dump(tokenized_sentences, outfile)\n",
    "        \n",
    "\n",
    "    def create_embeddings(self):\n",
    "        sentences = self.get_tokenized_sentences()\n",
    "        word2vec_model = Word2Vec(sentences, size=self.size, window=self.window, min_count=self.min_count, workers=self.workers)\n",
    "        word2index = dict([(k, v.index) for k, v in word2vec_model.wv.vocab.items()])\n",
    "        with open(self.path_vocabulary, \"w\") as output:\n",
    "            json.dump(word2index, output)\n",
    "        with open(self.path_word2vec_model, 'wb') as output:\n",
    "            pickle.dump(word2vec_model, output)\n",
    "    \n",
    "    def create_google_intersected_embeddings(self):\n",
    "        word2vec_model = self.get_model()\n",
    "        intersected_model = self.load_google_word2vec_model(word2vec_model) \n",
    "        with open(self.path_google_intersected, \"wb\") as output:\n",
    "            pickle.dump(intersected_model, output)\n",
    "            \n",
    "        \n",
    "    def index_sentences(self):\n",
    "        if not os.path.isfile(self.path_indexed_sentences):\n",
    "            tokenized_sentences = self.get_tokenized_sentences()\n",
    "            word2vec_model = self.get_intersected_model()\n",
    "            word2index, index2word = self.get_vocabulary() \n",
    "            indexed_sentences = [[word2index[word] for word in sent] for sent in tokenized_sentences]\n",
    "            with open(self.path_indexed_sentences, \"w\") as outfile:\n",
    "                json.dump(indexed_sentences, outfile)\n",
    "\n",
    "    def get_raw_text(self, dataset):\n",
    "        question_text = \"\"\n",
    "        for data in dataset:\n",
    "            for question in data['Question']:\n",
    "                question = self.noun_chunkers(question)\n",
    "                question = \"SQUADSTART \" + re.sub(r'[^\\w\\'\\+\\-\\=\\*\\s\\^]', '', question) + \" SQUADEND\"\n",
    "                yield question\n",
    "    \n",
    "    def load_embeddings(self):\n",
    "        print(\"Loading embeddings....\")\n",
    "        dataset = get_squad_wiki_data()\n",
    "        return self.get_raw_text(dataset)\n",
    "\n",
    "    # Returns word2Index and index2word\n",
    "    def get_vocabulary(self):\n",
    "        with open(self.path_vocabulary, 'r') as f:\n",
    "            data = json.load(f)\n",
    "        word2idx = data\n",
    "        idx2word = dict([(v, k) for k, v in data.items()])\n",
    "        return word2idx, idx2word\n",
    "\n",
    "    # Returns the pickled model\n",
    "    def get_model(self):\n",
    "        if not os.path.isfile(self.path_word2vec_model):\n",
    "            print(\"Creating Embeddings...\")\n",
    "            self.create_embeddings()\n",
    "        print(\"Loading Embeddings...\")\n",
    "        with open(self.path_word2vec_model,'rb') as output:\n",
    "            model = pickle.load(output)\n",
    "        return model\n",
    "    \n",
    "    def get_tokenized_sentences(self):\n",
    "        if not os.path.isfile(self.path_word_tokenized_sentence):\n",
    "            print(\"Creating Tokenized Sentences...\")\n",
    "            self.tokenize_sentences()\n",
    "        print(\"Loading Indexed Sentences...\")\n",
    "        with open(self.path_word_tokenized_sentence, \"r\") as file:\n",
    "            tokenized_sentences = json.load(file)\n",
    "        return tokenized_sentences\n",
    "\n",
    "    def get_indexed_sentences(self):\n",
    "        if not os.path.isfile(self.path_indexed_sentences):\n",
    "            print(\"Creating Indexed Sentences...\")\n",
    "            self.index_sentences()\n",
    "        print(\"Loading Indexed Sentences...\")\n",
    "        with open(self.path_indexed_sentences, 'r') as f:\n",
    "            indexed_sentences = json.load(f)\n",
    "        return indexed_sentences\n",
    "        \n",
    "    def load_google_word2vec_model(self, model):\n",
    "        print(\"INTERSECTING GOOGLES WORD2VEC MODEL WITH ORIGINAL WORD2VEC MODEL\")\n",
    "        model.intersect_word2vec_format(fname = '../model/GoogleNews-vectors-negative300.bin' , lockf = 1.0, binary = True)        \n",
    "        return model\n",
    "    \n",
    "    def get_intersected_model(self):\n",
    "        if not os.path.isfile(self.path_google_intersected):\n",
    "            self.create_google_intersected_embeddings()\n",
    "        with open(self.path_google_intersected, \"rb\") as output:\n",
    "            intersected_model = pickle.load(output)\n",
    "        return intersected_model\n",
    "            \n",
    "    def noun_chunkers(self, raw_text):\n",
    "        doc = nlp_en(raw_text)\n",
    "        for entity in doc.ents:\n",
    "            raw_text = raw_text.replace(str(entity), \"_\".join(str(entity).split()))\n",
    "        return raw_text\n",
    "    \n",
    "    def get_indexed_query(self, query):\n",
    "        query = self.noun_chunkers(query)\n",
    "        query = \"SQUADSTART \" + re.sub(r'[^\\w\\'\\+\\-\\=\\*\\s\\^]', '', query)\n",
    "        word_tokenized_query = word_tokenize(query.lower())\n",
    "        word2index, index2word = self.get_vocabulary()\n",
    "        indexed_query = [word2index[word] for word in word_tokenized_query if word in word2index.keys()]\n",
    "        return indexed_query\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "e = Embeddings(300, 4, 1, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'when was Super_Bowl 50. Indian_Space_Research_Organization is known as ISRO. World_Health_Organization.'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "e.noun_chunkers(\"when was Super Bowl 50. Indian Space Research Organization is known as ISRO. World  Health Organization.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../data/vocabulary_300_4_1_4.json'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-2745c42eed2d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_vocabulary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-2-058f925c0c99>\u001b[0m in \u001b[0;36mget_vocabulary\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     71\u001b[0m     \u001b[0;31m# Returns word2Index and index2word\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_vocabulary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m         \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath_vocabulary\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     74\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m         \u001b[0mword2idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../data/vocabulary_300_4_1_4.json'"
     ]
    }
   ],
   "source": [
    "a,b = e.get_vocabulary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# def preprocessor(self, raw_text, size, window, min_count, workers):  \n",
    "# #         tokenized_sentences = self.tokenize_sentence(raw_text)\n",
    "#         print(\"STOREING RAW TEXT AFTER REGEX AND WORD TOKENIZATION \")\n",
    "#         with open(\"../data/tokenized_sentences_after_regex.json\",\"w\") as outfile:\n",
    "#             json.dump(tokenized_sentences,outfile)\n",
    "        \n",
    "#         tokenized_pos_sentences = self.find_POS(tokenized_sentences)\n",
    "#         vocab = ['PUNCT','SYM','X','ADJ','VERB','CONJ','NUM','DET','ADV','PROPN','NOUN','PART','INTJ','CCONJ','SPACE','ADP','SCONJ','AUX', 'PRON']\n",
    "#         vocab = dict((word, index) for index, word in enumerate(vocab))\n",
    "#         with open(self.path_pos_indexed_vocabulary,'w') as outfile:\n",
    "#             json.dump(vocab, outfile)\n",
    "#         # initialize word2vector model\n",
    "#         model = Word2Vec(sentences = tokenized_sentences, size = size, window = window, min_count = min_count, workers = workers)\n",
    "#         intersected_model = self.load_google_word2vec_model(model)\n",
    "#         # finding out the vocabulary of raw_text with index     \n",
    "#         vocab = dict([(k, v.index) for k, v in intersected_model.wv.vocab.items()])\n",
    "#         # Storeing the vocab2index in a seperate file\n",
    "#         with open(self.path_indexed_vocabulary,'w') as outfile:\n",
    "#             json.dump(vocab, outfile)\n",
    "#          # finding gensim weights\n",
    "#         weights = intersected_model.wv.syn0\n",
    "#         # storeing weights in wordembeddings.npz file\n",
    "#         np.save(open(self.path_word_embeddings, 'wb'), weights)\n",
    "#         # dump the word2vec model in dump file word2vec_model\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
