{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re                                            # Regular python module for regex functions.\n",
    "import json                                          # Output into Json file.\n",
    "from wikipedia import WikipediaPage                  # Provides some attributes and function to read data from wikipedia. i.e. Titles, Summary, Context, Images.\n",
    "from wikipedia import DisambiguationError, PageError # Error thrown in case of Disambiguity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def normalize_passage(text_str):\n",
    "    \n",
    "    \"\"\" Remove some unnecessary characters from the data using regular python regex module.\"\"\"\n",
    "    \n",
    "    text_str = text_str.lower()\n",
    "    \n",
    "    text_str = re.sub(r\"(    )|(\\n)|(\\\\displaystyle)|(\\\\)|(Edit ===)|(Edit ==)|(===)|(==)\", \"\", text_str)\n",
    "\n",
    "    return text_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_from_wiki(titles):\n",
    "    \n",
    "    \"\"\" This function will except a list(list of Strings) of all the titles\n",
    "    and we will use these strings to get test from wikipedia.\"\"\"\n",
    "    \n",
    "    out_data_list = []                                                                   # List to append the dictionary elements(i.e. Required data with keys and values.) into one list.\n",
    "\n",
    "    for index, title in enumerate(titles):\n",
    "        out_data_dict = {'Title': title ,'Passage':'', \"Question\": [] ,\"Error\" : None }  # Will store our processed text into dictionary. {key:'Passage', value:'Text'}\n",
    "\n",
    "        try:\n",
    "            get_wiki_data = WikipediaPage(title = title)                                 # Get all the data from wikipedia.\n",
    "\n",
    "        except DisambiguationError:\n",
    "            # If there is any disambiguity in the Title name.\n",
    "            out_data_dict[\"Error\"] = (\"There is Disambigity in the title : \" + title + \". Please provide more precise title.\")\n",
    "        \n",
    "        except PageError:\n",
    "            # If no page found with the given title.\n",
    "            out_data_dict[\"Error\"] = (\"Page id \" + title + \" does not match any pages. Try another id!\")\n",
    "\n",
    "        if not out_data_dict[\"Error\"]:\n",
    "            # If there is no error then store the passage.\n",
    "            content_only = get_wiki_data.content             # Store main content into a variable.\n",
    "            processed_text = normalize_passage(content_only) # Process text using normalize_passge().\n",
    "            out_data_dict['Passage'] = processed_text        # Store received text into dictionary.\n",
    "            out_data_list.append(out_data_dict)              # Now append each dictionary into List.\n",
    "\n",
    "    return out_data_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def write_to_json(data):\n",
    "    \n",
    "    \"\"\" Convert list of dictionary into a json file and save it into data/wiki_text.json file. \"\"\"\n",
    "    \n",
    "    with open('../data/wiki_data.json', 'w', encoding='utf8') as outfile:\n",
    "        data_dump = json.dumps(data, indent=4, separators=(',', ': '))\n",
    "        outfile.write(data_dump)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "topics = ['Super Bowl 50', 'Warsaw', 'Normans', 'Nicola Tesla', 'Computational complexity theory', 'Teacher', 'Martin Luther',\n",
    "     'Southern California', 'Sky (United Kingdom)', 'Victoria (Australia)', 'Huguenot', 'Steam engine', 'Oxygen',\n",
    "    '1973 oil crisis', 'Apollo program', 'European Union Law', 'Amazon rainforest', 'Ctenophora','Fresno California',\n",
    "    'Packet switching', 'Black Death', 'Geology', 'Newcastle upon Tyne' , 'Victoria and Albert Museum', 'American Broadcasting Company',\n",
    "    'Genghis Khan', 'Pharmacy', 'Immune system', 'Civil disobedience', 'Construction' , 'Private school', 'Harvard University',\n",
    "    'Jacksonville, Florida', 'Economic inequality', 'Doctor Who', 'University of Chicago', 'Yuan dynasty', 'Kenya', 'Intergovernmental Panel on Climate Change',\n",
    "    'Chloroplast', 'Prime number', 'Rhine' , 'Scottish Parliament', 'Islamism', 'Imperialism', 'United Methodist Church', 'French and Indian War',\n",
    "    'Force']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "wiki_data = read_from_wiki(topics)\n",
    "write_to_json(wiki_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
