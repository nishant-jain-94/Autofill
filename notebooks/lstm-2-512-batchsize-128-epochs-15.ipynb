{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### importing require packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import json\n",
    "import os\n",
    "import numpy as np\n",
    "import sys\n",
    "import h5py\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.utils import simple_preprocess\n",
    "from keras.engine import Input\n",
    "from keras.layers import Embedding, merge\n",
    "from keras.models import Model\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import LSTM\n",
    "from keras.preprocessing import sequence\n",
    "from embeddings import Embeddings\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instantiate Embeddings "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading the embeddings from the cache\n"
     ]
    }
   ],
   "source": [
    "embeddings = Embeddings(100, 4, 1, 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### getting data from preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word2vec_weights = embeddings.get_weights()\n",
    "word2index, index2word = embeddings.get_vocabulary()\n",
    "word2vec_model = embeddings.get_model()\n",
    "tokenized_indexed_sentences = embeddings.get_tokenized_indexed_sentences()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### generating training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42047\n"
     ]
    }
   ],
   "source": [
    "window_size = 3\n",
    "vocab_size = len(word2index)\n",
    "print(vocab_size)\n",
    "#sorted(window_size,reverse=True)\n",
    "#sentence_max_length = max([len(sentence) for sentence in tokenized_indexed_sentence ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples :  67392\n"
     ]
    }
   ],
   "source": [
    "seq_in = []\n",
    "seq_out = []\n",
    "# generating dataset\n",
    "for sentence in tokenized_indexed_sentences:\n",
    "    for i in range(len(sentence)-window_size-1):\n",
    "        x = sentence[i:i + window_size]\n",
    "        y = sentence[i + window_size]\n",
    "        seq_in.append(x)#[]\n",
    "        seq_out.append(word2vec_weights[y])\n",
    "\n",
    "# converting seq_in and seq_out into numpy array\n",
    "seq_in = np.array(seq_in)\n",
    "seq_out = np.array(seq_out)\n",
    "n_samples = len(seq_in)\n",
    "print (\"Number of samples : \", n_samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_5 (Embedding)      (None, None, 100)         4204700   \n",
      "_________________________________________________________________\n",
      "lstm_9 (LSTM)                (None, None, 512)         1255424   \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, None, 512)         0         \n",
      "_________________________________________________________________\n",
      "lstm_10 (LSTM)               (None, 512)               2099200   \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 100)               51300     \n",
      "=================================================================\n",
      "Total params: 7,610,624\n",
      "Trainable params: 7,610,624\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Changes to the model to be done here\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=word2vec_weights.shape[0], output_dim=word2vec_weights.shape[1], weights=[word2vec_weights]))\n",
    "model.add(LSTM(512,return_sequences =True))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(512))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(word2vec_weights.shape[1], activation='relu'))\n",
    "model.compile(loss='mse', optimizer='adam',metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_weights_path = \"../weights/lstm-2-512-batchsize-128-epochs-15\"\n",
    "if not os.path.exists(model_weights_path):\n",
    "    os.makedirs(model_weights_path)\n",
    "checkpoint_path = model_weights_path + '/weights.{epoch:02d}-{val_acc:.2f}.hdf5'\n",
    "checkpoint = ModelCheckpoint(filepath=checkpoint_path, monitor='val_acc', verbose=1, save_best_only=False, mode='max')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1 samples, validate on 1 samples\n",
      "Epoch 1/1\n",
      "Epoch 00000: saving model to ../weights/lstm-2-512-batchsize-128-epochs-15/weights.00-0.00.hdf5\n",
      "1/1 [==============================] - 1s - loss: 0.3827 - acc: 0.0000e+00 - val_loss: 0.0514 - val_acc: 0.0000e+00\n"
     ]
    }
   ],
   "source": [
    "model_fit_summary = model.fit(seq_in, seq_out, epochs=15, verbose=1, validation_split=0.2, batch_size=128, callbacks=[checkpoint])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### model predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\" the name of\n",
      "minority minority minority minority minority minority minority minority minority minority "
     ]
    }
   ],
   "source": [
    "start =0\n",
    "pattern = list(seq_in[start])\n",
    "print(\"\\\"\",' '.join(index2word[index] for index in pattern))\n",
    "for i in range(10):\n",
    "    prediction = model.predict(np.array([pattern]))\n",
    "    pred_word = word2vec_model.similar_by_vector(prediction[0])[0][0]\n",
    "    sys.stdout.write(pred_word+\" \")\n",
    "    pattern.append(word2index[pred_word])\n",
    "    pattern = pattern[1:len(pattern)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(no_of_preds):\n",
    "    correct=0\n",
    "    wrong=0\n",
    "    random.seed(1)\n",
    "    for i in random.sample(range(0, seq_in.shape[0]), no_of_preds):\n",
    "        start = i\n",
    "        sentence = list(seq_in[start])\n",
    "        prediction = model.predict(np.array([sentence]))\n",
    "        pred_word = word2vec_model.similar_by_vector(prediction[0])[0][0]\n",
    "        next_word_index = list(seq_in[start+1])\n",
    "        next_word = index2word[next_word_index[-1]]\n",
    "        sim = word2vec_model.similarity(pred_word,next_word)\n",
    "        if (sim >= 0.8):\n",
    "            correct +=1\n",
    "        else : wrong +=1\n",
    "    print('correct: '+str(correct)+(' wrong: ')+str(wrong))\n",
    "    accur = float(correct/(correct+wrong))\n",
    "    return  accur\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_results = model_fit_summary.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_results.update(model_fit_summary.params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "correct: 60 wrong: 140\n"
     ]
    }
   ],
   "source": [
    "model_results[\"train_accuracy\"] = accuracy(seq_in.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3\n"
     ]
    }
   ],
   "source": [
    "# n = no. of predictions\n",
    "# accuracy = accuracy(400)\n",
    "print(model_results[\"train_accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_file_path = \"../weights/LSTM-2-512-Window-5-Batch-128-Epoch-10-Stateful/model_results.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(text_file_path, \"w\") as f:\n",
    "        json.dump(model_results, f)\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
