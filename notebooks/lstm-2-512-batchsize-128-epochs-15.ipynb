{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### importing require packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import json\n",
    "import os\n",
    "import numpy as np\n",
    "import sys\n",
    "import h5py\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.utils import simple_preprocess\n",
    "from keras.engine import Input\n",
    "from keras.layers import Embedding, merge\n",
    "from keras.models import Model\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import LSTM\n",
    "from keras.preprocessing import sequence\n",
    "from embeddings import Embeddings\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instantiate Embeddings "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading the embeddings from the cache\n"
     ]
    }
   ],
   "source": [
    "embeddings = Embeddings(100, 4, 1, 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### getting data from preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word2vec_weights = embeddings.get_weights()\n",
    "word2index, index2word = embeddings.get_vocabulary()\n",
    "word2vec_model = embeddings.get_model()\n",
    "tokenized_indexed_sentences = embeddings.get_tokenized_indexed_sentences()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### generating training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "170760\n"
     ]
    }
   ],
   "source": [
    "window_size = 3\n",
    "vocab_size = len(word2index)\n",
    "print(vocab_size)\n",
    "#sorted(window_size,reverse=True)\n",
    "#sentence_max_length = max([len(sentence) for sentence in tokenized_indexed_sentence ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples :  3381370\n"
     ]
    }
   ],
   "source": [
    "seq_in = []\n",
    "seq_out = []\n",
    "# generating dataset\n",
    "for sentence in tokenized_indexed_sentences:\n",
    "    for i in range(len(sentence)-window_size-1):\n",
    "        x = sentence[i:i + window_size]\n",
    "        y = sentence[i + window_size]\n",
    "        seq_in.append(x)#[]\n",
    "        seq_out.append(word2vec_weights[y])\n",
    "\n",
    "# converting seq_in and seq_out into numpy array\n",
    "seq_in = np.array(seq_in)\n",
    "seq_out = np.array(seq_out)\n",
    "n_samples = len(seq_in)\n",
    "print (\"Number of samples : \", n_samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, None, 100)         17076000  \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, None, 512)         1255424   \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, None, 512)         0         \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 512)               2099200   \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 100)               51300     \n",
      "=================================================================\n",
      "Total params: 20,481,924\n",
      "Trainable params: 20,481,924\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Changes to the model to be done here\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=word2vec_weights.shape[0], output_dim=word2vec_weights.shape[1], weights=[word2vec_weights]))\n",
    "model.add(LSTM(512,return_sequences =True))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(512))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(word2vec_weights.shape[1], activation='relu'))\n",
    "model.compile(loss='mse', optimizer='adam',metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_weights_path = \"../weights/lstm-2-512-batchsize-128-epochs-15\"\n",
    "if not os.path.exists(model_weights_path):\n",
    "    os.makedirs(model_weights_path)\n",
    "checkpoint_path = model_weights_path + '/weights.{epoch:02d}-{val_acc:.2f}.hdf5'\n",
    "checkpoint = ModelCheckpoint(filepath=checkpoint_path, monitor='val_acc', verbose=1, save_best_only=False, mode='max')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1 samples, validate on 1 samples\n",
      "Epoch 1/15\n",
      "Epoch 00000: saving model to ../weights/lstm-2-512-batchsize-128-epochs-15/weights.00-0.00.hdf5\n",
      "1/1 [==============================] - 2s - loss: 2.5437 - acc: 0.0000e+00 - val_loss: 4.5901 - val_acc: 0.0000e+00\n",
      "Epoch 2/15\n",
      "Epoch 00001: saving model to ../weights/lstm-2-512-batchsize-128-epochs-15/weights.01-0.00.hdf5\n",
      "1/1 [==============================] - 3s - loss: 2.4388 - acc: 1.0000 - val_loss: 4.5755 - val_acc: 0.0000e+00\n",
      "Epoch 3/15\n",
      "Epoch 00002: saving model to ../weights/lstm-2-512-batchsize-128-epochs-15/weights.02-0.00.hdf5\n",
      "1/1 [==============================] - 3s - loss: 2.2880 - acc: 1.0000 - val_loss: 4.5908 - val_acc: 0.0000e+00\n",
      "Epoch 4/15\n",
      "Epoch 00003: saving model to ../weights/lstm-2-512-batchsize-128-epochs-15/weights.03-0.00.hdf5\n",
      "1/1 [==============================] - 5s - loss: 2.1385 - acc: 1.0000 - val_loss: 4.6593 - val_acc: 0.0000e+00\n",
      "Epoch 5/15\n",
      "Epoch 00004: saving model to ../weights/lstm-2-512-batchsize-128-epochs-15/weights.04-0.00.hdf5\n",
      "1/1 [==============================] - 3s - loss: 1.9581 - acc: 1.0000 - val_loss: 4.8363 - val_acc: 0.0000e+00\n",
      "Epoch 6/15\n",
      "Epoch 00005: saving model to ../weights/lstm-2-512-batchsize-128-epochs-15/weights.05-0.00.hdf5\n",
      "1/1 [==============================] - 2s - loss: 1.8593 - acc: 1.0000 - val_loss: 5.0636 - val_acc: 0.0000e+00\n",
      "Epoch 7/15\n",
      "Epoch 00006: saving model to ../weights/lstm-2-512-batchsize-128-epochs-15/weights.06-0.00.hdf5\n",
      "1/1 [==============================] - 4s - loss: 1.8721 - acc: 1.0000 - val_loss: 5.1063 - val_acc: 0.0000e+00\n",
      "Epoch 8/15\n",
      "Epoch 00007: saving model to ../weights/lstm-2-512-batchsize-128-epochs-15/weights.07-0.00.hdf5\n",
      "1/1 [==============================] - 6s - loss: 1.8634 - acc: 1.0000 - val_loss: 5.0172 - val_acc: 0.0000e+00\n",
      "Epoch 9/15\n",
      "Epoch 00008: saving model to ../weights/lstm-2-512-batchsize-128-epochs-15/weights.08-0.00.hdf5\n",
      "1/1 [==============================] - 1s - loss: 1.8261 - acc: 1.0000 - val_loss: 4.8945 - val_acc: 0.0000e+00\n",
      "Epoch 10/15\n",
      "Epoch 00009: saving model to ../weights/lstm-2-512-batchsize-128-epochs-15/weights.09-0.00.hdf5\n",
      "1/1 [==============================] - 3s - loss: 1.8317 - acc: 1.0000 - val_loss: 4.7707 - val_acc: 0.0000e+00\n",
      "Epoch 11/15\n",
      "Epoch 00010: saving model to ../weights/lstm-2-512-batchsize-128-epochs-15/weights.10-0.00.hdf5\n",
      "1/1 [==============================] - 4s - loss: 1.8087 - acc: 1.0000 - val_loss: 4.6840 - val_acc: 0.0000e+00\n",
      "Epoch 12/15\n",
      "Epoch 00011: saving model to ../weights/lstm-2-512-batchsize-128-epochs-15/weights.11-0.00.hdf5\n",
      "1/1 [==============================] - 5s - loss: 1.7886 - acc: 1.0000 - val_loss: 4.6261 - val_acc: 0.0000e+00\n",
      "Epoch 13/15\n",
      "Epoch 00012: saving model to ../weights/lstm-2-512-batchsize-128-epochs-15/weights.12-0.00.hdf5\n",
      "1/1 [==============================] - 2s - loss: 1.7964 - acc: 1.0000 - val_loss: 4.5961 - val_acc: 0.0000e+00\n",
      "Epoch 14/15\n",
      "Epoch 00013: saving model to ../weights/lstm-2-512-batchsize-128-epochs-15/weights.13-0.00.hdf5\n",
      "1/1 [==============================] - 4s - loss: 1.7958 - acc: 1.0000 - val_loss: 4.5890 - val_acc: 0.0000e+00\n",
      "Epoch 15/15\n",
      "Epoch 00014: saving model to ../weights/lstm-2-512-batchsize-128-epochs-15/weights.14-0.00.hdf5\n",
      "1/1 [==============================] - 4s - loss: 1.7889 - acc: 1.0000 - val_loss: 4.5976 - val_acc: 0.0000e+00\n"
     ]
    }
   ],
   "source": [
    "model_fit_summary = model.fit(seq_in, seq_out, epochs=15, verbose=1, validation_split=0.2, batch_size=128, callbacks=[checkpoint])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### model predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\" super bowl 50\n",
      "was was was was was was was was was was "
     ]
    }
   ],
   "source": [
    "start =0\n",
    "pattern = list(seq_in[start])\n",
    "print(\"\\\"\",' '.join(index2word[index] for index in pattern))\n",
    "for i in range(10):\n",
    "    prediction = model.predict(np.array([pattern]))\n",
    "    pred_word = word2vec_model.similar_by_vector(prediction[0])[0][0]\n",
    "    sys.stdout.write(pred_word+\" \")\n",
    "    pattern.append(word2index[pred_word])\n",
    "    pattern = pattern[1:len(pattern)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def accuracy():\n",
    "    count = 0\n",
    "    correct = 0\n",
    "    for sub_sample_in, sub_sample_out in zip(seq_in[:5], seq_out[:5]):\n",
    "        ypred = model.predict_on_batch(np.expand_dims(sub_sample_in, axis=0))[0]\n",
    "        ytrue = sub_sample_out\n",
    "        pred_word = word2vec_model.similar_by_vector(ypred)[0][0]\n",
    "        true_word = word2vec_model.similar_by_vector(ytrue)[0][0]\n",
    "        similarity = word2vec_model.similarity(pred_word, true_word)\n",
    "        if similarity >= 0.85:\n",
    "            correct += 1\n",
    "        count += 1\n",
    "    print(\"Accuracy {0}\".format(correct/count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.27063617, -1.88592136, -4.60332966,  1.38032007, -3.50511742,\n",
       "        0.88621974, -0.93677127, -0.79569846, -3.14880967, -1.55611396,\n",
       "        1.76569748, -1.56616974, -1.98014486, -0.53436381,  0.75873446,\n",
       "       -0.45234403,  1.88288879,  0.71259999, -1.32030022, -0.96863592,\n",
       "        1.75420606, -0.84065282, -0.16819586, -1.39510345,  1.12865663,\n",
       "       -2.25509238, -1.10569012,  0.45855987,  1.38602078,  1.99246311,\n",
       "       -0.84836662,  1.90263259, -2.77856398, -1.62563455,  3.46212029,\n",
       "       -2.01167464,  3.70865345,  1.20190263, -0.10993056, -0.10728802,\n",
       "        1.09963083,  2.20335579, -1.30530989,  0.05224802, -0.61327147,\n",
       "       -2.37654543,  0.88630396,  1.29029536,  1.82292056, -0.13894933,\n",
       "       -0.37759912, -2.38005781,  0.1164887 , -0.08346321, -0.20829169,\n",
       "       -0.32738602,  1.08387029,  1.02966964, -1.70243227, -1.14024544,\n",
       "        0.93973899,  2.65973043, -3.04378986, -0.42154336,  2.16397929,\n",
       "        0.52978057, -0.14344002, -0.3650367 , -0.76624018,  1.28313541,\n",
       "        0.81438124, -1.13663208,  1.64061928, -2.34302354, -0.09878732,\n",
       "        0.13976237,  1.15372658, -2.66882372, -1.91749787,  1.4919076 ,\n",
       "        0.25347689,  1.3125174 ,  1.26598108,  1.53122878, -0.29941618,\n",
       "       -0.71239001, -0.6470781 ,  0.03714816,  1.96788037, -2.39959288,\n",
       "        0.43021342,  0.47725865,  2.3280437 ,  0.05286156, -2.84247327,\n",
       "       -1.7078588 , -0.40347713, -1.13539743, -0.94796562,  0.78792363], dtype=float32)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq_out[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_results = model_fit_summary.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_results.update(model_fit_summary.params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy 0.2\n"
     ]
    }
   ],
   "source": [
    "model_results[\"train_accuracy\"] = accuracy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "# n = no. of predictions\n",
    "# accuracy = accuracy(400)\n",
    "print(model_results[\"train_accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "text_file_path = \"../weights/lstm-2-512-batchsize-128-epochs-15/model_results.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(text_file_path, \"w\") as f:\n",
    "        json.dump(model_results, f)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
