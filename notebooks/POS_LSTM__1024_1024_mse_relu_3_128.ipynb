{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EMBEDDING(100,4,1,4) STARTED .....\n",
      "Loading the embeddings from the cache\n",
      "EMBEDDING(100,4,1,4) COMPLETED .....\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import h5py\n",
    "import datetime\n",
    "import json\n",
    "\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, LSTM\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from embeddings import Embeddings\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word_embedding_dimension = 100\n",
    "word_embedding_window_size = 4\n",
    "batch_size = 128 #BATCH_SIZE # 32, 64, 128\n",
    "epochs = 15 #EPOCH_SIZE # 10, 15, 30\n",
    "window_size = 3 #WINDOW_SIZE # 3, 4, 5\n",
    "accuracy_threshold = 0.85\n",
    "activation = 'relu' #ACTIVATION_FUNCTION # sigmoid, relu, softmax\n",
    "custom_accuracy = 0\n",
    "loss_function = 'mse' #LOSS_FUNCTION # mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_name = 'POS_LSTM_' + '_1024_1024_' + loss_function + \"_\" + activation + \"_\" + str(window_size) + \"_\" + str(batch_size) #MODEL_NAME #POS-LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading the embeddings from the cache\n"
     ]
    }
   ],
   "source": [
    "embeddings = Embeddings(word_embedding_dimension, word_embedding_window_size, 1, 4)\n",
    "tokenized_pos_sentences = embeddings.get_pos_categorical_indexed_sentences()\n",
    "pos2index, index2pos = embeddings.get_pos_vocabulary()\n",
    "no_of_unique_tags = len(pos2index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples :  68424\n"
     ]
    }
   ],
   "source": [
    "seq_in = []\n",
    "seq_out = []\n",
    "# generating dataset\n",
    "for sentence in tokenized_pos_sentences:\n",
    "    for i in range(len(sentence)-window_size-1):\n",
    "        x = sentence[i:i + window_size]\n",
    "        y = sentence[i + window_size]\n",
    "        seq_in.append(x)\n",
    "        seq_out.append(y)\n",
    "# converting seq_in and seq_out into numpy array\n",
    "seq_in = np.array(seq_in)\n",
    "seq_out = np.array(seq_out)\n",
    "n_samples = len(seq_in)\n",
    "print (\"Number of samples : \", n_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_data = seq_in\n",
    "y_data = seq_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 3, 1024)           4276224   \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 1024)              8392704   \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 19)                19475     \n",
      "=================================================================\n",
      "Total params: 12,688,403\n",
      "Trainable params: 12,688,403\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Changes to the model to be done here\n",
    "model = Sequential()\n",
    "model.add(LSTM(1024, input_shape=(x_data.shape[1], x_data.shape[2]), return_sequences=True))\n",
    "#model.add(Dropout(0.2))\n",
    "model.add(LSTM(1024))\n",
    "#model.add(Dropout(0.2))\n",
    "model.add(Dense(no_of_unique_tags, activation='relu'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam',metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_weights_path = \"../weights/\"+ model_name\n",
    "if not os.path.exists(model_weights_path):\n",
    "    os.makedirs(model_weights_path)\n",
    "checkpoint_path = model_weights_path + '/pos_weights.{epoch:02d}-{val_acc:.2f}.hdf5'\n",
    "checkpoint = ModelCheckpoint(filepath=checkpoint_path, monitor='val_acc', verbose=1, save_best_only=False, mode='max')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 75 samples, validate on 25 samples\n",
      "Epoch 1/5\n",
      "Epoch 00000: saving model to ../weights/POS_LSTMmse_0_relu_3_128/pos_weights.00-0.28.hdf5\n",
      "75/75 [==============================] - 0s - loss: 9.7370 - acc: 0.1867 - val_loss: 8.5557 - val_acc: 0.2800\n",
      "Epoch 2/5\n",
      "Epoch 00001: saving model to ../weights/POS_LSTMmse_0_relu_3_128/pos_weights.01-0.20.hdf5\n",
      "75/75 [==============================] - 0s - loss: 5.1961 - acc: 0.3867 - val_loss: 5.1698 - val_acc: 0.2000\n",
      "Epoch 3/5\n",
      "Epoch 00002: saving model to ../weights/POS_LSTMmse_0_relu_3_128/pos_weights.02-0.16.hdf5\n",
      "75/75 [==============================] - 0s - loss: 3.1124 - acc: 0.5067 - val_loss: 5.0623 - val_acc: 0.1600\n",
      "Epoch 4/5\n",
      "Epoch 00003: saving model to ../weights/POS_LSTMmse_0_relu_3_128/pos_weights.03-0.08.hdf5\n",
      "75/75 [==============================] - 0s - loss: 3.0978 - acc: 0.3067 - val_loss: 5.0362 - val_acc: 0.0800\n",
      "Epoch 5/5\n",
      "Epoch 00004: saving model to ../weights/POS_LSTMmse_0_relu_3_128/pos_weights.04-0.08.hdf5\n",
      "75/75 [==============================] - 0s - loss: 3.1098 - acc: 0.2400 - val_loss: 5.0249 - val_acc: 0.0800\n"
     ]
    }
   ],
   "source": [
    "model_fit_summary = model.fit(x_data, y_data, epochs=epochs, batch_size=batch_size, verbose=1, validation_split=0.25, callbacks=[checkpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct predictions:  19 \n",
      "Total Predictions:  100\n"
     ]
    }
   ],
   "source": [
    "check_ori = 0\n",
    "check_pre = 0\n",
    "counter = 0\n",
    "test_start = 0\n",
    "test_end = 10000\n",
    "list_for_hist_words = []\n",
    "list_for_hist_index = []\n",
    "list_for_hist_words_ori = []\n",
    "list_for_hist_index_ori = []\n",
    "for i in range(test_start, test_end):\n",
    "    test_no = i\n",
    "    to_predict = x_data[test_no:test_no+1]\n",
    "    y_ans = model.predict(to_predict)\n",
    "    \n",
    "    for word, corr_int in pos2index.items():\n",
    "        if corr_int == np.argmax(y_ans):\n",
    "            #print (\"pridicted: \",word, corr_int)\n",
    "            check_pre = corr_int\n",
    "            list_for_hist_words.append(word)\n",
    "            list_for_hist_index.append(corr_int)\n",
    "        if corr_int == np.argmax(y_data[test_no:test_no+1]):\n",
    "            #print (\"original: \",word, corr_int)\n",
    "            check_ori = corr_int\n",
    "            list_for_hist_words_ori.append(word)\n",
    "            list_for_hist_index_ori.append(corr_int)\n",
    "    if check_ori == check_pre :\n",
    "        counter += 1\n",
    "    #print('\\n')\n",
    "\n",
    "print(\"Correct predictions: \",counter, '\\nTotal Predictions: ',test_end - test_start)\n",
    "custom_accuracy = counter/(test_end-test_start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_results = model_fit_summary.history\n",
    "model_results.update(model_fit_summary.params)\n",
    "model_results[\"word_embedding_dimension\"] = word_embedding_dimension\n",
    "model_results[\"word_embedding_window_size\"] = word_embedding_window_size\n",
    "model_results[\"window_size\"] = window_size\n",
    "model_results[\"batch_size\"] = batch_size\n",
    "model_results[\"epochs\"] = epochs\n",
    "model_results[\"model_name\"] = model_name\n",
    "model_results[\"accuracy_threshold\"] = accuracy_threshold\n",
    "model_results[\"activation\"] = activation \n",
    "model_results[\"custom_accuracy\"] = custom_accuracy\n",
    "model_results[\"loss_function\"] = loss_function\n",
    "model_results[\"layers\"] = []\n",
    "model_results[\"dropouts\"] = []\n",
    "for layer in model.layers:\n",
    "    if hasattr(layer, \"units\"):\n",
    "        layer_summary = {}\n",
    "        layer_summary[\"units\"] = layer.get_config()[\"units\"]\n",
    "        layer_summary[\"name\"] = layer.name\n",
    "        model_results[\"layers\"].append(layer_summary)\n",
    "    if hasattr(layer, \"rate\"):\n",
    "        dropout_summary = {}\n",
    "        dropout_summary[\"rate\"] = layer.get_config()[\"rate\"]\n",
    "        model_results[\"dropouts\"].append(dropout_summary)\n",
    "text_file_path = \"../weights/{0}/model_results.json\".format(model_name)\n",
    "with open(text_file_path, \"w\") as f:\n",
    "        json.dump(model_results, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "# %matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# pos_vocab = [v for (k,v) in index2pos.items()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# plt.figure(figsize=(16,5))\n",
    "# plt.hist(list_for_hist_index, width=1, color='r', alpha=0.5)\n",
    "# plt.hist(list_for_hist_index_ori, width=1, color='b', alpha=0.5)\n",
    "# plt.xticks(range(len(pos_vocab)),pos_vocab, rotation='vertical')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# list_x = []\n",
    "# list_y = []\n",
    "# data_all = []\n",
    "# for i in range(0,1500):\n",
    "#     list_x.append((index2pos[np.argmax(x_data[i][0])], index2pos[np.argmax(x_data[i][1])]))\n",
    "#     list_y.append(index2pos[np.argmax(y_data[i])])\n",
    "#     data_all.append((str(list_x[i]),list_y[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# from nltk import ConditionalFreqDist as cfd\n",
    "# from nltk.collocations import *\n",
    "# import plotly.offline as plot\n",
    "# import plotly.graph_objs as go\n",
    "# plot.offline.init_notebook_mode(connected=True)\n",
    "# import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# cfd_res = cfd(data_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# df = pd.DataFrame(cfd_res).fillna(0)\n",
    "# mat = df.as_matrix()\n",
    "# #mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# trace = go.Heatmap(z = mat,\n",
    "#                    x=df.columns,\n",
    "#                    y=list(df.index))\n",
    "# data=[trace]\n",
    "# plot.iplot(data, filename='labelled-heatmap')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
