{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import json\n",
    "import os\n",
    "import numpy as np\n",
    "import sys\n",
    "import h5py\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.utils import simple_preprocess\n",
    "from keras.layers import Embedding\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import LSTM, Bidirectional\n",
    "from keras.preprocessing import sequence\n",
    "from intersect_embeddings import Embeddings\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "import random\n",
    "from itertools import groupby"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_name = \"bidirectional-GRU-2-256-dropout-0.2-300-batchsize-256-epochs-30-Sequence\"\n",
    "word_embedding_dimension = 300\n",
    "word_embedding_window_size = 4\n",
    "batch_size = 256 # 32, 64, 128\n",
    "epochs = 30 # 10, 15, 30\n",
    "window_size = \"None\" # 3, 4, 5\n",
    "accuracy_threshold = 1\n",
    "activation = 'softmax' # sigmoid, relu, softmax\n",
    "custom_accuracy = 0\n",
    "loss_function = 'cosine_proximity' # mse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instantiate Embeddings "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "embeddings = Embeddings(300, 4, 1, 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting data from preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec_model = embeddings.get_intersected_model()\n",
    "word2index, index2word = embeddings.get_vocabulary()\n",
    "word2vec_weights = word2vec_model.wv.syn0\n",
    "tokenized_indexed_sentences = embeddings.get_indexed_sentences()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word2index = {word:index+1 for word, index in word2index.items()}\n",
    "index2word = {index:word for word, index in word2index.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tokenized_indexed_sentences = [np.array(sentence) + 1 for sentence in tokenized_indexed_sentences if len(sentence) > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "new_weights = np.zeros((1, word2vec_weights.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "new_weights = np.append(new_weights, word2vec_weights, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "window_size = 5\n",
    "vocab_size = len(word2index)\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "maxlen = max([len(sentence) for sentence in tokenized_indexed_sentences])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tokenized_indexed_sentences = sequence.pad_sequences(tokenized_indexed_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "seq_in = np.zeros_like(tokenized_indexed_sentences)\n",
    "seq_out = np.zeros((tokenized_indexed_sentences.shape[0], tokenized_indexed_sentences.shape[1], 300))\n",
    "\n",
    "# Generating Dataset\n",
    "for index, sentence in enumerate(tokenized_indexed_sentences):\n",
    "    y = np.append(sentence[1:], np.array(sentence[len(sentence)-1]))\n",
    "    seq_in[index] += sentence\n",
    "    seq_out[index] += [new_weights[i] for i in y]\n",
    "\n",
    "n_samples = len(seq_in)\n",
    "print (\"Number of samples : \", n_samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Changes to the model to be done here\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=new_weights.shape[0], output_dim=new_weights.shape[1], weights=[new_weights], mask_zero=True))\n",
    "model.add(Bidirectional(GRU(256, return_sequences=True), merge_mode=\"ave\"))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Bidirectional(GRU(300, return_sequences=True), merge_mode=\"ave\"))\n",
    "model.compile(loss=loss_function, optimizer='adam',metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Weights Directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_weights_path = \"../weights/\" + model_name\n",
    "if not os.path.exists(model_weights_path):\n",
    "    os.makedirs(model_weights_path)\n",
    "checkpoint_path = model_weights_path + '/weights.{epoch:02d}.hdf5'\n",
    "checkpoint = ModelCheckpoint(filepath=checkpoint_path, verbose=1, save_best_only=False, mode='max')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_fit_summary = model.fit(seq_in, seq_out, epochs=epochs, verbose=1, batch_size=batch_size, callbacks=[checkpoint])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "start = 0\n",
    "sentence_test = \"In which regions in particular did\"\n",
    "indexed_sentences = embeddings.get_indexed_query(sentence_test)\n",
    "print(\"indexed_sentences \",indexed_sentences)\n",
    "sent = np.array(indexed_sentences)\n",
    "#pattern = list(seq_in[start])\n",
    "pattern = list(sent)\n",
    "print(\"\\\"\",' '.join(index2word[index] for index in pattern))\n",
    "for i in range(10):\n",
    "    prediction = model.predict(np.array([pattern]))\n",
    "    pred_word = word2vec_model.similar_by_vector(prediction[0][prediction.shape[1] - 1])[0][0]\n",
    "    sys.stdout.write(pred_word+\" \")\n",
    "    pattern.append(word2index[pred_word])\n",
    "    pattern = pattern[:len(pattern)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_results = model_fit_summary.history\n",
    "model_results.update(model_fit_summary.params)\n",
    "model_results[\"word_embedding_dimension\"] = word_embedding_dimension\n",
    "model_results[\"word_embedding_window_size\"] = word_embedding_window_size\n",
    "model_results[\"window_size\"] = window_size\n",
    "model_results[\"batch_size\"] = batch_size\n",
    "model_results[\"epochs\"] = epochs\n",
    "model_results[\"model_name\"] = model_name\n",
    "model_results[\"accuracy_threshold\"] = accuracy_threshold\n",
    "model_results[\"activation\"] = activation \n",
    "model_results[\"custom_accuracy\"] = custom_accuracy\n",
    "model_results[\"loss_function\"] = loss_function\n",
    "model_results[\"layers\"] = []\n",
    "model_results[\"dropouts\"] = []\n",
    "for layer in model.layers:\n",
    "    if hasattr(layer, \"units\"):\n",
    "        layer_summary = {}\n",
    "        layer_summary[\"units\"] = layer.get_config()[\"units\"]\n",
    "        layer_summary[\"name\"] = layer.name\n",
    "        model_results[\"layers\"].append(layer_summary)\n",
    "    if hasattr(layer, \"rate\"):\n",
    "        dropout_summary = {}\n",
    "        dropout_summary[\"rate\"] = layer.get_config()[\"rate\"]\n",
    "        model_results[\"dropouts\"].append(dropout_summary)\n",
    "text_file_path = \"../weights/{0}/model_results.json\".format(model_name)\n",
    "with open(text_file_path, \"w\") as f:\n",
    "        json.dump(model_results, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
