{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### importing require packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EMBEDDING(100,4,1,4) STARTED .....\n",
      "Loading the embeddings from the cache\n",
      "EMBEDDING(100,4,1,4) COMPLETED .....\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import json\n",
    "import os\n",
    "import numpy as np\n",
    "import sys\n",
    "import h5py\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.utils import simple_preprocess\n",
    "from keras.engine import Input\n",
    "from keras.layers import Embedding, merge\n",
    "from keras.models import Model\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import LSTM\n",
    "from keras.preprocessing import sequence\n",
    "from embeddings import Embeddings\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instantiate Embeddings "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading the embeddings from the cache\n"
     ]
    }
   ],
   "source": [
    "embeddings = Embeddings(100, 4, 1, 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### getting data from preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word2vec_weights = embeddings.get_weights()\n",
    "word2index, index2word = embeddings.get_vocabulary()\n",
    "word2vec_model = embeddings.get_model()\n",
    "tokenized_indexed_sentences = embeddings.get_tokenized_indexed_sentences()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### generating training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42047\n"
     ]
    }
   ],
   "source": [
    "window_size = 5\n",
    "vocab_size = len(word2index)\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples :  47295\n"
     ]
    }
   ],
   "source": [
    "seq_in = []\n",
    "seq_out = []\n",
    "# generating dataset\n",
    "for sentence in tokenized_indexed_sentences:\n",
    "    for i in range(len(sentence)-window_size-1):\n",
    "        x = sentence[i:i + window_size]\n",
    "        y = sentence[i + window_size]\n",
    "        seq_in.append(x)#[]\n",
    "        seq_out.append(word2vec_weights[y])\n",
    "\n",
    "# converting seq_in and seq_out into numpy array\n",
    "seq_in = np.array(seq_in)\n",
    "seq_out = np.array(seq_out)\n",
    "n_samples = len(seq_in)\n",
    "print (\"Number of samples : \", n_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(47295, 5)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq_in.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, None, 100)         4204700   \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, None, 1024)        4608000   \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, None, 1024)        0         \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 512)               3147776   \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 100)               51300     \n",
      "=================================================================\n",
      "Total params: 12,011,776\n",
      "Trainable params: 12,011,776\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Changes to the model to be done here\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=word2vec_weights.shape[0], output_dim=word2vec_weights.shape[1], weights=[word2vec_weights]))\n",
    "model.add(LSTM(1024,return_sequences =True))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(512))\n",
    "#model.add(Dropout(0.2))\n",
    "model.add(Dense(word2vec_weights.shape[1], activation='relu'))\n",
    "model.load_weights(\"../weights/lstm-2-1024-512-batchsize-128-epochs-25/weights.24-0.22.hdf5\")\n",
    "model.compile(loss='mse', optimizer='adam',metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_weights_path = \"../weights/lstm-2-1024-512-batchsize-128-epochs-25\"\n",
    "if not os.path.exists(model_weights_path):\n",
    "    os.makedirs(model_weights_path)\n",
    "checkpoint_path = model_weights_path + '/weights.{epoch:02d}-{val_acc:.2f}.hdf5'\n",
    "checkpoint = ModelCheckpoint(filepath=checkpoint_path, monitor='val_acc', verbose=1, save_best_only=False, mode='max')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model_fit_summary = model.fit(seq_in, seq_out, epochs=25, verbose=1, validation_split=0.2, batch_size=128, callbacks=[checkpoint])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### model predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "indexed_sentences  [[33, 1, 1612, 6, 198, 6646, 3, 0, 6035, 2113]]\n",
      "\" how many times have the\n",
      "corrientes billaut handing self-interest profitabl circulated iconodules profitabl profitabl eoka "
     ]
    }
   ],
   "source": [
    "start = 100\n",
    "sentence_test = \"At what pressure is water heated in the Rankine cycle\"\n",
    "indexed_sentences = embeddings.tokenize_index_sentence(sentence_test)\n",
    "print(\"indexed_sentences \",indexed_sentences)\n",
    "pattern = indexed_sentences\n",
    "pattern = list(seq_in[start])\n",
    "print(\"\\\"\",' '.join(index2word[index] for index in pattern))\n",
    "for i in range(10):\n",
    "    prediction = model.predict(np.array([pattern]))\n",
    "    pred_word = word2vec_model.similar_by_vector(prediction[0])[0][0]\n",
    "    sys.stdout.write(pred_word+\" \")\n",
    "    pattern.append(word2index[pred_word])\n",
    "    pattern = pattern[1:len(pattern)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def accuracy():\n",
    "    count = 0\n",
    "    correct = 0\n",
    "    for sub_sample_in, sub_sample_out in zip(seq_in[:5], seq_out[:5]):\n",
    "        ypred = model.predict_on_batch(np.expand_dims(sub_sample_in, axis=0))[0]\n",
    "        ytrue = sub_sample_out\n",
    "        pred_word = word2vec_model.similar_by_vector(ypred)[0][0]\n",
    "        true_word = word2vec_model.similar_by_vector(ytrue)[0][0]\n",
    "        similarity = word2vec_model.similarity(pred_word, true_word)\n",
    "        if similarity >= 0.85:\n",
    "            correct += 1\n",
    "        count += 1\n",
    "    print(\"Accuracy {0}\".format(correct/count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.27063617, -1.88592136, -4.60332966,  1.38032007, -3.50511742,\n",
       "        0.88621974, -0.93677127, -0.79569846, -3.14880967, -1.55611396,\n",
       "        1.76569748, -1.56616974, -1.98014486, -0.53436381,  0.75873446,\n",
       "       -0.45234403,  1.88288879,  0.71259999, -1.32030022, -0.96863592,\n",
       "        1.75420606, -0.84065282, -0.16819586, -1.39510345,  1.12865663,\n",
       "       -2.25509238, -1.10569012,  0.45855987,  1.38602078,  1.99246311,\n",
       "       -0.84836662,  1.90263259, -2.77856398, -1.62563455,  3.46212029,\n",
       "       -2.01167464,  3.70865345,  1.20190263, -0.10993056, -0.10728802,\n",
       "        1.09963083,  2.20335579, -1.30530989,  0.05224802, -0.61327147,\n",
       "       -2.37654543,  0.88630396,  1.29029536,  1.82292056, -0.13894933,\n",
       "       -0.37759912, -2.38005781,  0.1164887 , -0.08346321, -0.20829169,\n",
       "       -0.32738602,  1.08387029,  1.02966964, -1.70243227, -1.14024544,\n",
       "        0.93973899,  2.65973043, -3.04378986, -0.42154336,  2.16397929,\n",
       "        0.52978057, -0.14344002, -0.3650367 , -0.76624018,  1.28313541,\n",
       "        0.81438124, -1.13663208,  1.64061928, -2.34302354, -0.09878732,\n",
       "        0.13976237,  1.15372658, -2.66882372, -1.91749787,  1.4919076 ,\n",
       "        0.25347689,  1.3125174 ,  1.26598108,  1.53122878, -0.29941618,\n",
       "       -0.71239001, -0.6470781 ,  0.03714816,  1.96788037, -2.39959288,\n",
       "        0.43021342,  0.47725865,  2.3280437 ,  0.05286156, -2.84247327,\n",
       "       -1.7078588 , -0.40347713, -1.13539743, -0.94796562,  0.78792363], dtype=float32)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#seq_out[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_results = model_fit_summary.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_results.update(model_fit_summary.params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy 0.2\n"
     ]
    }
   ],
   "source": [
    "model_results[\"train_accuracy\"] = accuracy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'val_acc': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 'do_validation': True, 'metrics': ['loss', 'acc', 'val_loss', 'val_acc'], 'samples': 1, 'acc': [0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], 'epochs': 15, 'val_loss': [4.5900764465332031, 4.5754680633544922, 4.5907573699951172, 4.6593356132507324, 4.8363356590270996, 5.0636167526245117, 5.1062922477722168, 5.0171608924865723, 4.8945426940917969, 4.7707195281982422, 4.683952808380127, 4.6261215209960938, 4.5961475372314453, 4.588953971862793, 4.5975632667541504], 'batch_size': 128, 'train_accuracy': None, 'verbose': 1, 'loss': [2.5437352657318115, 2.4388267993927002, 2.2880129814147949, 2.1385490894317627, 1.9580711126327515, 1.8592877388000488, 1.8720529079437256, 1.863358736038208, 1.8261202573776245, 1.831728458404541, 1.8086607456207275, 1.788567066192627, 1.796411395072937, 1.7958264350891113, 1.7889491319656372]}\n"
     ]
    }
   ],
   "source": [
    "# n = no. of predictions\n",
    "# accuracy = accuracy(400)\n",
    "#print(model_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "text_file_path = \"../weights/lstm-2-1024-512-batchsize-128-epochs-25/model_results.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(text_file_path, \"w\") as f:\n",
    "        json.dump(model_results, f)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
