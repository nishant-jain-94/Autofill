{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# N-gram-Approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### importing require packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "import re\n",
    "from nltk.util import ngrams\n",
    "from nltk.probability import ConditionalFreqDist, ConditionalProbDist, MLEProbDist\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(\"../data/squad_wiki_data.json\",\"r\") as outfile:\n",
    "    dataset = json.load(outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "questions = dataset[0]['Question']\n",
    "questions = ' '.join(questions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generatng n_grams-bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cpdist_training_n_plus_1_grams(training_passage, n_plus_one):\n",
    "    \n",
    "    ## removing special character and symbols and converting to lower case\n",
    "    training_passage = re.sub(r\"(^\\w|\\')\", ' ', training_passage).lower()\n",
    "    \n",
    "    ## tokenizing the sanitized passage\n",
    "    words = nltk.word_tokenize(training_passage)\n",
    "    \n",
    "    cfdist_list = []\n",
    "    cpdist_list = []\n",
    "    \n",
    "    ## generating cpdist and n_grams for n_plus_one to bigrams\n",
    "    for n in range(n_plus_one,1,-1):\n",
    "        ## generating n_plus_one_grams and converting into list\n",
    "        n_grams_generated = list(ngrams(words, n))\n",
    "        ## converting into (n_gram, n+1 words) for prediction\n",
    "        n_grams_for_predict = [(n_gram[:-1],n_gram[-1]) for n_gram in n_grams_generated] \n",
    "        \n",
    "        ## calculating conditionalfrequency for all n_grams\n",
    "        cfdist = ConditionalFreqDist(n_grams_for_predict)\n",
    "        \n",
    "        ## calculating conditional probablitlity of next word for all n_grams\n",
    "        cpdist = ConditionalProbDist(cfdist,MLEProbDist)\n",
    "        \n",
    "        cfdist_list.append(cfdist)\n",
    "        cpdist_list.append(cpdist)\n",
    "    \n",
    "    return cpdist_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "cp_list = cpdist_training_n_plus_1_grams(questions, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#len(cp_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict the next word function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def predict_next_using_n_grams(n_grams, cpdist_list, mode=\"nsent\"):\n",
    "    # n_gram = tuple of n_words #input to the function\n",
    "    len_n_grams = len(n_grams)\n",
    "    print(n_grams)\n",
    "    # to end the recursion\n",
    "    if(len_n_grams==0):return #no prediction available\n",
    "    \n",
    "    #handlind sentence with length more than the n_grams generated\n",
    "    if len_n_grams > len(cpdist_list): \n",
    "        n_grams = n_grams[-len(cpdist_list):]\n",
    "        len_n_grams = len(n_grams)\n",
    "    \n",
    "    # possible predictions\n",
    "    possible_pred = list(cpdist_list[len(cpdist_list)-len_n_grams][n_grams].samples())\n",
    "    # number of possible prediciton for the provided n_grams\n",
    "    n_possible_pred = len(possible_pred)\n",
    "    \n",
    "    if n_possible_pred!=0:\n",
    "        \n",
    "        if(mode == 'nword'):\n",
    "            if(n_possible_pred == 1):\n",
    "            #pred = possible_pred[0]\n",
    "            #tuple(list(n_grams).append(pred)[1:])\n",
    "                return possible_pred[0]\n",
    "            else:\n",
    "            ## trying combining multiple probability\n",
    "                return '\\n'.join(possible_pred[0:5])\n",
    "            \n",
    "            \n",
    "        if(mode == 'nsent'):\n",
    "            if(n_possible_pred == 1):\n",
    "                pred_words = []\n",
    "                n_grams_list = list(n_grams)\n",
    "                while(possible_pred[0]!='?'):\n",
    "                    pred_words.append(possible_pred[0])\n",
    "                    n_grams_list.append(possible_pred[0])                \n",
    "                    n_grams_list = n_grams_list[1:]\n",
    "                    possible_pred = list(cpdist_list[len(cpdist_list)-len_n_grams][tuple(n_grams_list)].samples())\n",
    "                pred_words.append('?')\n",
    "                return ' '.join(pred_words)\n",
    "            \n",
    "            else:\n",
    "            ## returning 2 sentence\n",
    "                preds = []\n",
    "                pred1, pred2 = [],[]\n",
    "                n_grams_list1, n_grams_list2 = list(n_grams), list(n_grams)\n",
    "                \n",
    "                possible_pred1 = possible_pred[0]\n",
    "                possible_pred2 = possible_pred[1]\n",
    "                \n",
    "                while(possible_pred1!='?'):\n",
    "                    pred1.append(possible_pred1)\n",
    "                    n_grams_list1.append(possible_pred1)                \n",
    "                    n_grams_list1 = n_grams_list1[1:]\n",
    "                    possible_pred1 = list(cpdist_list[len(cpdist_list)-len_n_grams][tuple(n_grams_list1)].samples())[0]\n",
    "                    \n",
    "                pred1.append('?')\n",
    "                print(pred1)\n",
    "                preds.append(' '.join(pred1))\n",
    "                \n",
    "                while(possible_pred2!='?'):\n",
    "                    pred2.append(possible_pred2)\n",
    "                    n_grams_list2.append(possible_pred2)                \n",
    "                    n_grams_list2 = n_grams_list2[1:]\n",
    "                    possible_pred2 = list(cpdist_list[len(cpdist_list)-len_n_grams][tuple(n_grams_list2)].samples())[0]\n",
    "                    \n",
    "                pred2.append('?')\n",
    "                preds.append(' '.join(pred2))\n",
    "                print(pred2)\n",
    "                return ' '.join(preds)\n",
    "            \n",
    "    else:\n",
    "        #if prediciton is not available for the provided n_grams backoff\n",
    "        n_grams = n_grams[1:]\n",
    "        return predict_next_using_n_grams(n_grams, cpdist_list,mode)\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_predcition(n_grams, mode=\"nsent\"):\n",
    "    n_grams = re.sub(\"[^\\w\\']\", ' ', n_grams).lower()\n",
    "    n_grams = tuple(nltk.word_tokenize(n_grams))\n",
    "    ans = ' '.join(n_grams)+'\\n'\n",
    "    ans+= predict_next_using_n_grams(n_grams, cp_list, mode)\n",
    "    return ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('who', 'is', 'the')\n",
      "['``', 'chocolate', 'district', \"''\", 'located', '?']\n",
      "['largest', 'worldwide', ',', 'located', 'in', 'jakarta', ',', 're-open', '?']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"`` chocolate district '' located ? largest worldwide , located in jakarta , re-open ?\""
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_next_using_n_grams(('who' ,'is' ,'the'),cp_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#t = (\"who\",\"is\",\"the\",\"oldest\")\n",
    "#' '.join(t)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#\"who\",\"is\",\"the\",\"oldest\",\"quarterback\",\"to\",\"play\",\"in\",\"a\",\"super\",\"bowl\",\"by\",\"the\",\"time\",\"they\",\"reached\",\"super\",\"bowl\",\"50\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#t =(\"who\",\"is\",\"the\",\"oldest\",\"quarterback\")\n",
    "## check why this is still predicting\n",
    "\n",
    "#'super', 'bowl', '50', '?']\n",
    "#13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#t = list(t)\n",
    "#t.append('a')\n",
    "#t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#type(t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### keeping only a-z and 0-9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#questions = re.sub(r\"(^\\w|\\')\", ' ', questions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#questions = questions.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#words = nltk.word_tokenize(questions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### n-grams : 4 grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#_4grams = ngrams(words,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#_4grams = list(_4grams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#_4grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#cfdist = ConditionalFreqDist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#_4grams_to_predict_3grams =  [(_4gram[:-1],_4gram[-1]) for _4gram in _4grams]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#_4grams_to_predict_3grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#cfdist = ConditionalFreqDist(_4grams_to_predict_3grams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cfdist\n",
    "# ('able',): FreqDist({'to': 1}),"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cfdist[(\"stamp\",)]\n",
    "#did not stamp out the "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#cpdist = ConditionalProbDist(cfdist,MLEProbDist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#nltk.probability.MLEProbDist.freqdist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#def samples(tup):\n",
    "#    return list(cpdist[tup].samples())\n",
    "    #print(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#samples((\"stamp\", \"out\", \"the\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cpdist[(\"empire\", \"during\", \"much\")].prob('of')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#list(range(4,1,-1))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
