{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### importing require packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import json\n",
    "import os\n",
    "import numpy as np\n",
    "import sys\n",
    "import h5py\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.utils import simple_preprocess\n",
    "from keras.engine import Input\n",
    "from keras.layers import Embedding, merge\n",
    "from keras.models import Model\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import LSTM\n",
    "from keras.preprocessing import sequence\n",
    "from embeddings import Embeddings\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instantiate Embeddings "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "embeddings = Embeddings(100, 4, 1, 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### getting data from preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word2vec_weights = embeddings.get_weights()\n",
    "word2index, index2word = embeddings.get_vocabulary()\n",
    "word2vec_model = embeddings.get_model()\n",
    "tokenized_indexed_sentences = embeddings.get_tokenized_indexed_sentences()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### generating training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "170760\n"
     ]
    }
   ],
   "source": [
    "window_size = 3\n",
    "vocab_size = len(word2index)\n",
    "print(vocab_size)\n",
    "#sorted(window_size,reverse=True)\n",
    "#sentence_max_length = max([len(sentence) for sentence in tokenized_indexed_sentence ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples :  3381370\n"
     ]
    }
   ],
   "source": [
    "seq_in = []\n",
    "seq_out = []\n",
    "# generating dataset\n",
    "for sentence in tokenized_indexed_sentences:\n",
    "    for i in range(len(sentence)-window_size-1):\n",
    "        x = sentence[i:i + window_size]\n",
    "        y = sentence[i + window_size]\n",
    "        seq_in.append(x)#[]\n",
    "        seq_out.append(word2vec_weights[y])\n",
    "\n",
    "# converting seq_in and seq_out into numpy array\n",
    "seq_in = np.array(seq_in)\n",
    "seq_out = np.array(seq_out)\n",
    "n_samples = len(seq_in)\n",
    "print (\"Number of samples : \", n_samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, None, 100)         17076000  \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, None, 512)         1255424   \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, None, 512)         0         \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 512)               2099200   \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 100)               51300     \n",
      "=================================================================\n",
      "Total params: 20,481,924\n",
      "Trainable params: 20,481,924\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Changes to the model to be done here\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=word2vec_weights.shape[0], output_dim=word2vec_weights.shape[1], weights=[word2vec_weights]))\n",
    "model.add(LSTM(512,return_sequences =True))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(512))\n",
    "model.add(Dense(word2vec_weights.shape[1], activation='relu'))\n",
    "model.compile(loss='mse', optimizer='adam',metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_weights_path = \"../weights/lstm-2-256-batchsize-128-epochs-10\"\n",
    "if not os.path.exists(model_weights_path):\n",
    "    os.makedirs(model_weights_path)\n",
    "checkpoint_path = model_weights_path + '/weights.{epoch:02d}-{val_acc:.2f}.hdf5'\n",
    "checkpoint = ModelCheckpoint(filepath=checkpoint_path, monitor='val_acc', verbose=1, save_best_only=False, mode='max')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 800 samples, validate on 200 samples\n",
      "Epoch 1/10\n",
      "768/800 [===========================>..] - ETA: 0s - loss: 1.1152 - acc: 0.1211Epoch 00000: saving model to ../weights/lstm-2-256-batchsize-128-epochs-10/weights.00-0.12.hdf5\n",
      "800/800 [==============================] - 4s - loss: 1.1103 - acc: 0.1237 - val_loss: 0.7468 - val_acc: 0.1150\n",
      "Epoch 2/10\n",
      "768/800 [===========================>..] - ETA: 0s - loss: 1.1015 - acc: 0.2018Epoch 00001: saving model to ../weights/lstm-2-256-batchsize-128-epochs-10/weights.01-0.07.hdf5\n",
      "800/800 [==============================] - 5s - loss: 1.0945 - acc: 0.2012 - val_loss: 0.7360 - val_acc: 0.0650\n",
      "Epoch 3/10\n",
      "768/800 [===========================>..] - ETA: 0s - loss: 1.0802 - acc: 0.2031Epoch 00002: saving model to ../weights/lstm-2-256-batchsize-128-epochs-10/weights.02-0.10.hdf5\n",
      "800/800 [==============================] - 4s - loss: 1.0879 - acc: 0.2012 - val_loss: 0.7438 - val_acc: 0.1000\n",
      "Epoch 4/10\n",
      "768/800 [===========================>..] - ETA: 0s - loss: 1.0799 - acc: 0.1771Epoch 00003: saving model to ../weights/lstm-2-256-batchsize-128-epochs-10/weights.03-0.11.hdf5\n",
      "800/800 [==============================] - 4s - loss: 1.0818 - acc: 0.1787 - val_loss: 0.7465 - val_acc: 0.1050\n",
      "Epoch 5/10\n",
      "768/800 [===========================>..] - ETA: 0s - loss: 1.0712 - acc: 0.2018Epoch 00004: saving model to ../weights/lstm-2-256-batchsize-128-epochs-10/weights.04-0.07.hdf5\n",
      "800/800 [==============================] - 4s - loss: 1.0766 - acc: 0.2025 - val_loss: 0.7569 - val_acc: 0.0700\n",
      "Epoch 6/10\n",
      "768/800 [===========================>..] - ETA: 0s - loss: 1.0731 - acc: 0.2057Epoch 00005: saving model to ../weights/lstm-2-256-batchsize-128-epochs-10/weights.05-0.07.hdf5\n",
      "800/800 [==============================] - 4s - loss: 1.0715 - acc: 0.2038 - val_loss: 0.7534 - val_acc: 0.0650\n",
      "Epoch 7/10\n",
      "768/800 [===========================>..] - ETA: 0s - loss: 1.0473 - acc: 0.1875Epoch 00006: saving model to ../weights/lstm-2-256-batchsize-128-epochs-10/weights.06-0.09.hdf5\n",
      "800/800 [==============================] - 4s - loss: 1.0652 - acc: 0.1862 - val_loss: 0.7734 - val_acc: 0.0900\n",
      "Epoch 8/10\n",
      "768/800 [===========================>..] - ETA: 0s - loss: 1.0537 - acc: 0.2005Epoch 00007: saving model to ../weights/lstm-2-256-batchsize-128-epochs-10/weights.07-0.07.hdf5\n",
      "800/800 [==============================] - 4s - loss: 1.0575 - acc: 0.2038 - val_loss: 0.7615 - val_acc: 0.0650\n",
      "Epoch 9/10\n",
      "768/800 [===========================>..] - ETA: 0s - loss: 1.0418 - acc: 0.2044Epoch 00008: saving model to ../weights/lstm-2-256-batchsize-128-epochs-10/weights.08-0.10.hdf5\n",
      "800/800 [==============================] - 4s - loss: 1.0471 - acc: 0.2038 - val_loss: 0.7786 - val_acc: 0.1000\n",
      "Epoch 10/10\n",
      "768/800 [===========================>..] - ETA: 0s - loss: 1.0277 - acc: 0.2201Epoch 00009: saving model to ../weights/lstm-2-256-batchsize-128-epochs-10/weights.09-0.10.hdf5\n",
      "800/800 [==============================] - 4s - loss: 1.0398 - acc: 0.2175 - val_loss: 0.7708 - val_acc: 0.1000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f9b00c3b5c0>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(seq_in[:1000], seq_out[:1000], epochs=10, verbose=1, validation_split=0.2, batch_size=128, callbacks=[checkpoint])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### model predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = 97\n",
    "pattern = list(seq_in[start])\n",
    "print(\"\\\"\",' '.join(index2word[index] for index in pattern))\n",
    "for i in range(10):\n",
    "    prediction = model.predict(np.array([pattern]))\n",
    "    pred_word = word2vec_model.similar_by_vector(prediction[0])[0][0]\n",
    "    sys.stdout.write(pred_word+\" \")\n",
    "    pattern.append(word2index[pred_word])\n",
    "    pattern = pattern[1:len(pattern)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def accuracy(no_of_preds):\n",
    "    correct=0\n",
    "    wrong=0\n",
    "    random.seed(1)\n",
    "    for i in random.sample(range(0, seq_in.shape[0]), no_of_preds):\n",
    "        start = i\n",
    "        sentence = list(seq_in[start])\n",
    "        prediction = model.predict(np.array([sentence]))\n",
    "        pred_word = word2vec_model.similar_by_vector(prediction[0])[0][0]\n",
    "        next_word_index = list(seq_in[start+1])\n",
    "        next_word = index2word[next_word_index[-1]]\n",
    "        sim = word2vec_model.similarity(pred_word,next_word)\n",
    "        if (sim >= 0.6):\n",
    "            correct +=1\n",
    "        else : wrong +=1\n",
    "    print('correct: '+str(correct)+(' wrong: ')+str(wrong))\n",
    "    accur = float(correct/(correct+wrong))\n",
    "    print('accuracy = ',float(accur))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# n = no. of predictions\n",
    "accuracy(400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
