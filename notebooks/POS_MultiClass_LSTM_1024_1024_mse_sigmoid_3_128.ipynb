{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import h5py\n",
    "import datetime\n",
    "import json\n",
    "import pandas as pd\n",
    "import itertools \n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, LSTM\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from embeddings import Embeddings\n",
    "from keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word_embedding_dimension = 300\n",
    "word_embedding_window_size = 4\n",
    "batch_size = 128 # 32, 64, 128\n",
    "epochs = 15 # 10, 15, 30\n",
    "window_size = 3 # 3, 4, 5\n",
    "accuracy_threshold = 0.85\n",
    "activation = 'sigmoid' # sigmoid, relu, softmax\n",
    "custom_accuracy = 0\n",
    "loss_function = 'mse' # mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_name = 'POS_MultiClass_LSTM' + '_1024_1024_' + loss_function + \"_\" + activation + \"_\" + str(window_size) + \"_\" + str(batch_size) #MODEL_NAME #POS-LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/word_tokenized_sentence_300_4_1_4.json', 'r') as myfile:\n",
    "    raw_data = json.load(myfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading the embeddings from the cache\n"
     ]
    }
   ],
   "source": [
    "embeddings = Embeddings(word_embedding_dimension, word_embedding_window_size, 1, 4)\n",
    "pos2index, index2pos = embeddings.get_pos_vocabulary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = embeddings.find_POS(raw_data) #find_POS(raw_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "whole_test_data = [word for sent in test_data for word in sent]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "new_data = []\n",
    "for i in range(len(whole_test_data)-window_size-1):\n",
    "    x = whole_test_data[i:i + window_size + 1]\n",
    "    new_data.append(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "new_data = [[data[:3], data[3]] for data in new_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vocab = ['PUNCT','SYM','X','ADJ','VERB','CONJ','NUM','DET','ADV','PROPN','NOUN','PART','INTJ','CCONJ','SPACE','ADP','SCONJ','AUX', 'PRON']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data.sort()\n",
    "new_seq_in = []\n",
    "new_seq_out = []\n",
    "for i,j in itertools.groupby(new_data, lambda x: x[0]):\n",
    "    ex = set(list(zip(*list(j)))[1])\n",
    "    \n",
    "    inputs = [to_categorical(pos2index[x_pos], num_classes = len(vocab)) for x_pos in i]\n",
    "    new_seq_in_each = [each[0] for each in inputs] \n",
    "    new_seq_in.append(new_seq_in_each)\n",
    "    \n",
    "    outputs = [(to_categorical(pos2index[y_pos], num_classes = len(vocab))).tolist()[0] for y_pos in ex]\n",
    "    new_seq_out_each = [each for each in outputs]\n",
    "    new_seq_out_each = np.sum(new_seq_out_each, axis=0)\n",
    "    new_seq_out.append(new_seq_out_each)\n",
    "    \n",
    "\n",
    "new_seq_in = np.array(new_seq_in)\n",
    "new_seq_out = np.array(new_seq_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Changes to the model to be done here\n",
    "model = Sequential()\n",
    "model.add(LSTM(1024, input_shape=(new_seq_in.shape[1], new_seq_in.shape[2]), return_sequences=True))\n",
    "#model.add(Dropout(0.2))\n",
    "model.add(LSTM(1024))\n",
    "#model.add(Dropout(0.2))\n",
    "model.add(Dense(len(vocab), activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam',metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_weights_path = \"../weights/\"+ model_name\n",
    "if not os.path.exists(model_weights_path):\n",
    "    os.makedirs(model_weights_path)\n",
    "checkpoint_path = model_weights_path + '/pos_weights.{epoch:02d}-{val_acc:.2f}.hdf5'\n",
    "checkpoint = ModelCheckpoint(filepath=checkpoint_path, monitor='val_acc', verbose=1, save_best_only=False, mode='max')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_fit_summary = model.fit(new_seq_in, new_seq_out, epochs=epochs, batch_size=batch_size, verbose=1, validation_split=0.25, callbacks=[checkpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_ori = 0\n",
    "check_pre = 0\n",
    "counter = 0\n",
    "test_start = 0\n",
    "test_end = 100\n",
    "list_for_hist_words = []\n",
    "list_for_hist_index = []\n",
    "list_for_hist_words_ori = []\n",
    "list_for_hist_index_ori = []\n",
    "for i in range(test_start, test_end):\n",
    "    test_no = i\n",
    "    to_predict = new_seq_in[test_no:test_no+1]\n",
    "    y_ans = model.predict(to_predict)\n",
    "    \n",
    "    for word, corr_int in pos2index.items():\n",
    "        if corr_int == np.argmax(y_ans):\n",
    "            #print (\"pridicted: \",word, corr_int)\n",
    "            check_pre = corr_int\n",
    "            list_for_hist_words.append(word)\n",
    "            list_for_hist_index.append(corr_int)\n",
    "        if corr_int == np.argmax(y_data[test_no:test_no+1]):\n",
    "            #print (\"original: \",word, corr_int)\n",
    "            check_ori = corr_int\n",
    "            list_for_hist_words_ori.append(word)\n",
    "            list_for_hist_index_ori.append(corr_int)\n",
    "    if check_ori == check_pre :\n",
    "        counter += 1\n",
    "    #print('\\n')\n",
    "\n",
    "print(\"Correct predictions: \",counter, '\\nTotal Predictions: ',test_end - test_start)\n",
    "custom_accuracy = counter/(test_end-test_start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_results = model_fit_summary.history\n",
    "model_results.update(model_fit_summary.params)\n",
    "model_results[\"word_embedding_dimension\"] = word_embedding_dimension\n",
    "model_results[\"word_embedding_window_size\"] = word_embedding_window_size\n",
    "model_results[\"window_size\"] = window_size\n",
    "model_results[\"batch_size\"] = batch_size\n",
    "model_results[\"epochs\"] = epochs\n",
    "model_results[\"model_name\"] = model_name\n",
    "model_results[\"accuracy_threshold\"] = accuracy_threshold\n",
    "model_results[\"activation\"] = activation \n",
    "model_results[\"custom_accuracy\"] = custom_accuracy\n",
    "model_results[\"loss_function\"] = loss_function\n",
    "model_results[\"layers\"] = []\n",
    "model_results[\"dropouts\"] = []\n",
    "for layer in model.layers:\n",
    "    if hasattr(layer, \"units\"):\n",
    "        layer_summary = {}\n",
    "        layer_summary[\"units\"] = layer.get_config()[\"units\"]\n",
    "        layer_summary[\"name\"] = layer.name\n",
    "        model_results[\"layers\"].append(layer_summary)\n",
    "    if hasattr(layer, \"rate\"):\n",
    "        dropout_summary = {}\n",
    "        dropout_summary[\"rate\"] = layer.get_config()[\"rate\"]\n",
    "        model_results[\"dropouts\"].append(dropout_summary)\n",
    "text_file_path = \"../weights/{0}/model_results.json\".format(model_name)\n",
    "with open(text_file_path, \"w\") as f:\n",
    "        json.dump(model_results, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
