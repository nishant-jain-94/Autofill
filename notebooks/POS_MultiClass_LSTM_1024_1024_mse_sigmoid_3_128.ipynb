{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 867,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import h5py\n",
    "import datetime\n",
    "import json\n",
    "import pandas as pd\n",
    "import itertools \n",
    "from keras.utils import to_categorical\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, LSTM\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from embeddings import Embeddings\n",
    "from keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 921,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word_embedding_dimension = 300\n",
    "word_embedding_window_size = 4\n",
    "batch_size = 128 # 32, 64, 128\n",
    "epochs = 15 # 10, 15, 30\n",
    "window_size = 3 # 3, 4, 5\n",
    "accuracy_threshold = 0.85\n",
    "activation = 'sigmoid' # sigmoid, relu, softmax\n",
    "custom_accuracy = 0\n",
    "loss_function = 'mse' # mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 919,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_name = 'POS_MultiClass_LSTM' + '_1024_1024_' + loss_function + \"_\" + activation + \"_\" + str(window_size) + \"_\" + str(batch_size) #MODEL_NAME #POS-LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 920,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'POS_MultiClass_LSTM_1024_1024_mse_sigmoid_3_128'"
      ]
     },
     "execution_count": 920,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 906,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/word_tokenized_sentence_300_4_1_4.json', 'r') as myfile:\n",
    "    raw_data = json.load(myfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 882,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_data = embeddings.tokenize_sentence(raw_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 912,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_data = embeddings.find_POS(raw_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 916,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "whole_test_data = [word for sent in test_data for word in sent]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 917,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "new_data = []\n",
    "for i in range(len(whole_test_data)-window_size-1):\n",
    "    x = whole_test_data[i:i + window_size + 1]\n",
    "    new_data.append(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 886,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "new_data = [[data[:3], data[3]] for data in new_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 887,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vocab = ['PUNCT','SYM','X','ADJ','VERB','CONJ','NUM','DET','ADV','PROPN','NOUN','PART','INTJ','CCONJ','SPACE','ADP','SCONJ','AUX', 'PRON']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 888,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data.sort()\n",
    "new_seq_in = []\n",
    "new_seq_out = []\n",
    "for i,j in itertools.groupby(new_data, lambda x: x[0]):\n",
    "    ex = set(list(zip(*list(j)))[1])\n",
    "    \n",
    "    inputs = [to_categorical(pos2index[x_pos], num_classes = len(vocab)) for x_pos in i]\n",
    "    new_seq_in_each = [each[0] for each in inputs] \n",
    "    new_seq_in.append(new_seq_in_each)\n",
    "    \n",
    "    outputs = [(to_categorical(pos2index[y_pos], num_classes = len(vocab))).tolist()[0] for y_pos in ex]\n",
    "    new_seq_out_each = [each for each in outputs]\n",
    "    new_seq_out_each = np.sum(new_seq_out_each, axis=0)\n",
    "    new_seq_out.append(new_seq_out_each)\n",
    "    \n",
    "\n",
    "new_seq_in = np.array(new_seq_in)\n",
    "new_seq_out = np.array(new_seq_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 890,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1860"
      ]
     },
     "execution_count": 890,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(new_seq_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 898,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_13 (LSTM)               (None, 3, 512)            1089536   \n",
      "_________________________________________________________________\n",
      "lstm_14 (LSTM)               (None, 512)               2099200   \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 19)                9747      \n",
      "=================================================================\n",
      "Total params: 3,198,483\n",
      "Trainable params: 3,198,483\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Changes to the model to be done here\n",
    "model = Sequential()\n",
    "model.add(LSTM(1024, input_shape=(new_seq_in.shape[1], new_seq_in.shape[2]), return_sequences=True))\n",
    "#model.add(Dropout(0.2))\n",
    "model.add(LSTM(1024))\n",
    "#model.add(Dropout(0.2))\n",
    "model.add(Dense(len(vocab), activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam',metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 899,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_weights_path = \"../weights/\"+ model_name\n",
    "if not os.path.exists(model_weights_path):\n",
    "    os.makedirs(model_weights_path)\n",
    "checkpoint_path = model_weights_path + '/pos_weights.{epoch:02d}-{val_acc:.2f}.hdf5'\n",
    "checkpoint = ModelCheckpoint(filepath=checkpoint_path, monitor='val_acc', verbose=1, save_best_only=False, mode='max')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 900,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 51318 samples, validate on 17106 samples\n",
      "Epoch 1/2\n",
      "51200/51318 [============================>.] - ETA: 0s - loss: 1.6921 - acc: 0.4050Epoch 00000: saving model to ../weights/POS_Template_Seq_in_test_1024_1024_mse_sigmoid_3_128/pos_weights.00-0.45.hdf5\n",
      "51318/51318 [==============================] - 164s - loss: 1.6917 - acc: 0.4051 - val_loss: 1.5452 - val_acc: 0.4548\n",
      "Epoch 2/2\n",
      "51200/51318 [============================>.] - ETA: 0s - loss: 1.5692 - acc: 0.4396Epoch 00001: saving model to ../weights/POS_Template_Seq_in_test_1024_1024_mse_sigmoid_3_128/pos_weights.01-0.46.hdf5\n",
      "51318/51318 [==============================] - 162s - loss: 1.5687 - acc: 0.4398 - val_loss: 1.5089 - val_acc: 0.4641\n"
     ]
    }
   ],
   "source": [
    "model_fit_summary = model.fit(x_data, y_data, epochs=epochs, batch_size=batch_size, verbose=1, validation_split=0.25, callbacks=[checkpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 902,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct predictions:  33 \n",
      "Total Predictions:  100\n"
     ]
    }
   ],
   "source": [
    "check_ori = 0\n",
    "check_pre = 0\n",
    "counter = 0\n",
    "test_start = 0\n",
    "test_end = 100\n",
    "list_for_hist_words = []\n",
    "list_for_hist_index = []\n",
    "list_for_hist_words_ori = []\n",
    "list_for_hist_index_ori = []\n",
    "for i in range(test_start, test_end):\n",
    "    test_no = i\n",
    "    to_predict = new_seq_in[test_no:test_no+1]\n",
    "    y_ans = model.predict(to_predict)\n",
    "    \n",
    "    for word, corr_int in pos2index.items():\n",
    "        if corr_int == np.argmax(y_ans):\n",
    "            #print (\"pridicted: \",word, corr_int)\n",
    "            check_pre = corr_int\n",
    "            list_for_hist_words.append(word)\n",
    "            list_for_hist_index.append(corr_int)\n",
    "        if corr_int == np.argmax(y_data[test_no:test_no+1]):\n",
    "            #print (\"original: \",word, corr_int)\n",
    "            check_ori = corr_int\n",
    "            list_for_hist_words_ori.append(word)\n",
    "            list_for_hist_index_ori.append(corr_int)\n",
    "    if check_ori == check_pre :\n",
    "        counter += 1\n",
    "    #print('\\n')\n",
    "\n",
    "print(\"Correct predictions: \",counter, '\\nTotal Predictions: ',test_end - test_start)\n",
    "custom_accuracy = counter/(test_end-test_start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_results = model_fit_summary.history\n",
    "model_results.update(model_fit_summary.params)\n",
    "model_results[\"word_embedding_dimension\"] = word_embedding_dimension\n",
    "model_results[\"word_embedding_window_size\"] = word_embedding_window_size\n",
    "model_results[\"window_size\"] = window_size\n",
    "model_results[\"batch_size\"] = batch_size\n",
    "model_results[\"epochs\"] = epochs\n",
    "model_results[\"model_name\"] = model_name\n",
    "model_results[\"accuracy_threshold\"] = accuracy_threshold\n",
    "model_results[\"activation\"] = activation \n",
    "model_results[\"custom_accuracy\"] = custom_accuracy\n",
    "model_results[\"loss_function\"] = loss_function\n",
    "model_results[\"layers\"] = []\n",
    "model_results[\"dropouts\"] = []\n",
    "for layer in model.layers:\n",
    "    if hasattr(layer, \"units\"):\n",
    "        layer_summary = {}\n",
    "        layer_summary[\"units\"] = layer.get_config()[\"units\"]\n",
    "        layer_summary[\"name\"] = layer.name\n",
    "        model_results[\"layers\"].append(layer_summary)\n",
    "    if hasattr(layer, \"rate\"):\n",
    "        dropout_summary = {}\n",
    "        dropout_summary[\"rate\"] = layer.get_config()[\"rate\"]\n",
    "        model_results[\"dropouts\"].append(dropout_summary)\n",
    "text_file_path = \"../weights/{0}/model_results.json\".format(model_name)\n",
    "with open(text_file_path, \"w\") as f:\n",
    "        json.dump(model_results, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
