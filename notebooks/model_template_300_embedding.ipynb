{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### importing require packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import json\n",
    "import os\n",
    "import numpy as np\n",
    "import sys\n",
    "import h5py\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.utils import simple_preprocess\n",
    "from keras.engine import Input\n",
    "from keras.layers import Embedding, merge\n",
    "from keras.models import Model\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import LSTM\n",
    "from keras.preprocessing import sequence\n",
    "from embeddings import Embeddings\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instantiate Embeddings "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading embeddings....\n",
      "Loading Squad Data\n",
      "Loading the embeddings from the cache\n",
      "Starting tokenized, pos squad data.....\n",
      "Combining Squad Data\n",
      "Creating Tokenized Squad Corpus\n",
      "Creating Tokenized Squad Corpus\n"
     ]
    }
   ],
   "source": [
    "word_embedding_dimension = 300\n",
    "word_embedding_window_size = 4\n",
    "embeddings = Embeddings(word_embedding_dimension, word_embedding_window_size, 1, 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### getting data from preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word2vec_weights = embeddings.get_weights()\n",
    "word2index, index2word = embeddings.get_vocabulary()\n",
    "word2vec_model = embeddings.get_model()\n",
    "tokenized_indexed_sentences = embeddings.get_tokenized_indexed_sentences()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### generating training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42049\n"
     ]
    }
   ],
   "source": [
    "window_size = 5\n",
    "vocab_size = len(word2index)\n",
    "print(vocab_size)\n",
    "#sorted(window_size,reverse=True)\n",
    "#sentence_max_length = max([len(sentence) for sentence in tokenized_indexed_sentence ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples :  610848\n"
     ]
    }
   ],
   "source": [
    "seq_in = []\n",
    "seq_out = []\n",
    "# generating dataset\n",
    "for sentence in tokenized_indexed_sentences:\n",
    "    for i in range(len(sentence)-window_size-1):\n",
    "        x = sentence[i:i + window_size]\n",
    "        y = sentence[i + window_size]\n",
    "        seq_in.append(x)#[]\n",
    "        seq_out.append(word2vec_weights[y])\n",
    "\n",
    "# converting seq_in and seq_out into numpy array\n",
    "seq_in = np.array(seq_in)\n",
    "seq_out = np.array(seq_out)\n",
    "n_samples = len(seq_in)\n",
    "print (\"Number of samples : \", n_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern = list(seq_in[97])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\" would super bowl 50 have\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "NoneType"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(print(\"\\\"\",' '.join(index2word[index] for index in pattern)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(42049, 300)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2vec_weights.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, None, 300)         12614700  \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, None, 512)         1665024   \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, None, 512)         0         \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 512)               2099200   \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 300)               153900    \n",
      "=================================================================\n",
      "Total params: 16,532,824\n",
      "Trainable params: 16,532,824\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Changes to the model to be done here\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=word2vec_weights.shape[0], output_dim=word2vec_weights.shape[1], weights=[word2vec_weights]))\n",
    "model.add(LSTM(512, return_sequences=True))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(512))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(word2vec_weights.shape[1], activation='sigmoid'))\n",
    "model.compile(loss='mse', optimizer='adam',metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_weights_path = \"../weights/LSTM_1_Layer\"\n",
    "if not os.path.exists(model_weights_path):\n",
    "    os.makedirs(model_weights_path)\n",
    "checkpoint_path = model_weights_path + '/weights.{epoch:02d}-{val_acc:.2f}.hdf5'\n",
    "checkpoint = ModelCheckpoint(filepath=checkpoint_path, monitor='val_acc', verbose=1, save_best_only=False, mode='max')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 488678 samples, validate on 122170 samples\n",
      "Epoch 1/1\n",
      "116480/488678 [======>.......................] - ETA: 2093s - loss: 0.1635 - acc: 0.0764"
     ]
    }
   ],
   "source": [
    "model_fit_summary = model.fit(seq_in, seq_out, epochs=1, verbose=1, validation_split=0.2, batch_size=256, callbacks=[checkpoint])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### model predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "start = 97\n",
    "pattern = list(seq_in[start])\n",
    "print(\"\\\"\",' '.join(index2word[index] for index in pattern))\n",
    "for i in range(10):\n",
    "    prediction = model.predict(np.array([pattern]))\n",
    "    pred_word = word2vec_model.similar_by_vector(prediction[0])[0][0]\n",
    "    sys.stdout.write(pred_word)\n",
    "    pattern.append(word2index[pred_word])\n",
    "    pattern = pattern[1:len(pattern)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def accuracy():\n",
    "    count = 0\n",
    "    correct = 0\n",
    "    for sub_sample_in, sub_sample_out in zip(seq_in[:5], seq_out[:5]):\n",
    "        ypred = model.predict_on_batch(np.expand_dims(sub_sample_in, axis=0))[0]\n",
    "        ytrue = sub_sample_out\n",
    "        pred_word = word2vec_model.similar_by_vector(ypred)[0][0]\n",
    "        true_word = word2vec_model.similar_by_vector(ytrue)[0][0]\n",
    "        similarity = word2vec_model.similarity(pred_word, true_word)\n",
    "        if similarity >= 0.85:\n",
    "            correct += 1\n",
    "        count += 1\n",
    "    print(\"Accuracy {0}\".format(correct/count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# n = no. of predictions\n",
    "accuracy(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_results = model_fit_summary.history\n",
    "model_results.update(model_fit_summary.params)\n",
    "model_results[\"word_embedding_dimension\"] = word_embedding_dimension\n",
    "model_results[\"word_embedding_window_size\"] = word_embedding_window_size\n",
    "model_results[\"window_size\"] = window_size\n",
    "\n",
    "model_results[\"train_accuracy\"] = accuracy()\n",
    "text_file_path = \"../weights/lstm-2-512-batchsize-128-epochs-15/model_results.json\"\n",
    "with open(text_file_path, \"w\") as f:\n",
    "        json.dump(model_results, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
