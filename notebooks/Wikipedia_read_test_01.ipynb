{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re                                            # Regular python module for regex functions.\n",
    "import json                                          # Output into Json file.\n",
    "import io\n",
    "from wikipedia import WikipediaPage                  # Provides some attributes and function to read data from wikipedia. i.e. Titles, Summary, Context, Images.\n",
    "from wikipedia import DisambiguationError, PageError # Error thrown in case of Disambiguity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def normalize_passage(text_str):\n",
    "    \n",
    "    \"\"\" Remove some unnecessary characters from the data using regular python regex\"\"\"\n",
    "\n",
    "    text_str = re.sub(r\"(    )\", '',text_str) ## Remove unwanted spaces.\n",
    "    text_str = re.sub(r\"(\\n)\", '',text_str) ## Remove newline chars.\n",
    "    text_str = re.sub(r\"(\\\\displaystyle)\", '',text_str) \n",
    "    text_str = re.sub(r\"(\\\\)\", '',text_str)\n",
    "    text_str = re.sub(r'(Edit ==)','',text_str)\n",
    "    text_str = re.sub(r'(Edit ===)','',text_str)\n",
    "    text_str = re.sub(r'(==)','',text_str)\n",
    "    text_str = re.sub(r'(===)','',text_str)\n",
    "\n",
    "    return text_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_from_wiki(titles):\n",
    "    \n",
    "    \"\"\" This function will except a list(list of Strings) of all the titles \n",
    "    and we will use these strings to get test from wikipedia.\"\"\"\n",
    "    \n",
    "    out_data_list = []\n",
    "    \n",
    "    for title in titles:\n",
    "        \n",
    "        no_error = True\n",
    "        \n",
    "        out_data_dict = {'Title': title ,'Passage':'', \"Question\": [] ,\"Error\" : None }  ## Will store our processed text into dictionary. {key:'Passage', value:'Text'}\n",
    "        \n",
    "        try:\n",
    "            get_wiki_data = WikipediaPage(title = title)  ## Get all the data from wikipedia.\n",
    "            \n",
    "        except DisambiguationError:\n",
    "            \n",
    "            ## If there is any disambiguity in the Title name.\n",
    "            error_str = (\"There is Disambigity in the title : \"+title+ \". Please provide more precise title.\")\n",
    "            no_error = False ## If there is any error set it False.\n",
    "            #return error_str\n",
    "        \n",
    "        except PageError:\n",
    "            \n",
    "            ## If no page found with the given title.\n",
    "            error_str = (\"Page id \"+title+\" does not match any pages. Try another id!\")\n",
    "            no_error = False ## If there is any error set it False.\n",
    "            #return error_str\n",
    "        \n",
    "        if no_error:\n",
    "            \n",
    "            content_only = get_wiki_data.content # Get only main content.\n",
    "        \n",
    "            processed_text = normalize_passage(content_only) ## Process text using normalize_passge function.\n",
    "        \n",
    "            out_data_dict['Passage'] = processed_text ## Store received text into dictionary.\n",
    "        \n",
    "            out_data_list.append(out_data_dict) ## Now append each dictionary into List.\n",
    "            \n",
    "        else : \n",
    "            \n",
    "            out_data_dict['Error'] = error_str\n",
    "            print (title)\n",
    "            print(error_str)\n",
    "        \n",
    "    return out_data_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def to_json(list_of_titles):\n",
    "    \"\"\" Convert list of dictionary into a json file. \"\"\"\n",
    "    \n",
    "    with io.open('./../data/wiki_text.json', 'w', encoding='utf8') as outfile:\n",
    "        str_ = json.dumps(read_from_wiki(list_of_titles), indent=4, separators=(',', ': '))\n",
    "        outfile.write(str_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "topics = ['Super Bowl 50', 'Warsaw', 'Normans', 'Nicola Tesla', 'Computational complexity theory', 'Teacher', 'Martin Luther',\n",
    "     'Southern California', 'Sky (United Kingdom)', 'Victoria (Australia)', 'Huguenot', 'Steam engine', 'Oxygen',\n",
    "    '1973 oil crisis', 'Apollo program', 'European Union Law', 'Amazon rainforest', 'Ctenophora','Fresno California', \n",
    "    'Packet switching', 'Black Death', 'Geology', 'Newcastle upon Tyne' , 'Victoria and Albert Museum', 'American Broadcasting Company',\n",
    "    'Genghis Khan', 'Pharmacy', 'Immune system', 'Civil disobedience', 'Construction' , 'Private school', 'Harvard University',\n",
    "    'Jacksonville, Florida', 'Economic inequality', 'Doctor Who', 'University of Chicago', 'Yuan dynasty', 'Kenya', 'Intergovernmental Panel on Climate Change',\n",
    "    'Chloroplast', 'Prime number', 'Rhine' , 'Scottish Parliament', 'Islamism', 'Imperialism', 'United Methodist Church', 'French and Indian War',\n",
    "    'Force']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_json(topics)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
